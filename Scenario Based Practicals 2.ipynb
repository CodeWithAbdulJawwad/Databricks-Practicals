{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df12314d-12ac-4701-a48c-055ac4b5457b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗ 1. How do you manage and process unstructured data in your data pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd2aba5-5351-403e-a03f-388172b3589f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample Unstructured JSON Log Data (Simulating Incoming Log Files)\n",
    "json_data = [\n",
    "    ('{\"timestamp\":\"2024-02-25 12:00:01\", \"user\":\"Alice\", \"event\":\"login\", \"ip\":\"192.168.1.1\"}',),\n",
    "    ('{\"timestamp\":\"2024-02-25 12:05:10\", \"user\":\"Bob\", \"event\":\"purchase\", \"amount\": 250.50}',),\n",
    "    ('{\"timestamp\":\"2024-02-25 12:10:20\", \"user\":\"Charlie\", \"event\":\"logout\", \"ip\":\"192.168.1.2\"}',)\n",
    "]\n",
    "\n",
    "columns = [\"raw_json\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(json_data, columns)\n",
    "\n",
    "# Step 1: Extracting Key Fields from JSON (Schema Inference)\n",
    "extracted_df = df.select(\n",
    "    json_tuple(col(\"raw_json\"), \"timestamp\", \"user\", \"event\", \"ip\", \"amount\").alias(\"timestamp\", \"user\", \"event\", \"ip\", \"amount\")\n",
    ")\n",
    "\n",
    "# Step 2: Converting Data Types (Ensure Numerical Fields Are Correct)\n",
    "structured_df = extracted_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# Show Processed Data\n",
    "structured_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e97f993-4798-4cb8-9342-102e7bf47acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗2. Can you discuss your experience with performance tuning in big data frameworks like Spark or Hadoop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97545616-3ff2-4faf-9363-53e474cb5cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Large Orders Dataset (Simulating Big Data)\n",
    "orders_data = [\n",
    "    (1, 101, \"2024-02-25\", 500),\n",
    "    (2, 102, \"2024-02-24\", 300),\n",
    "    (3, 103, \"2024-02-23\", 700),\n",
    "    (4, 104, \"2024-02-22\", 150),\n",
    "]\n",
    "\n",
    "orders_columns = [\"order_id\", \"customer_id\", \"order_date\", \"amount\"]\n",
    "\n",
    "# Sample Small Customers Dataset (For Join Optimization)\n",
    "customers_data = [\n",
    "    (101, \"Alice\"),\n",
    "    (102, \"Bob\"),\n",
    "    (103, \"Charlie\"),\n",
    "    (104, \"David\"),\n",
    "]\n",
    "\n",
    "customers_columns = [\"customer_id\", \"customer_name\"]\n",
    "\n",
    "# Creating DataFrames\n",
    "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
    "customers_df = spark.createDataFrame(customers_data, customers_columns)\n",
    "\n",
    "# Step 1: Optimize Join with Broadcast (Avoid Shuffle)\n",
    "optimized_join_df = orders_df.join(broadcast(customers_df), \"customer_id\")\n",
    "\n",
    "# Step 2: Use Partitioning to Speed Up Queries\n",
    "partitioned_df = orders_df.repartition(\"order_date\")  # Partition by order_date for faster filtering\n",
    "\n",
    "# Show Optimized Data\n",
    "optimized_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebd4e425-ab71-42e9-9683-d84dc7b4ff86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗3. What strategies do you use to ensure data quality in your ETL processes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66a1ecf-b65f-4466-91a2-fc52af01937c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Data with Quality Issues\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"alice@example.com\"),\n",
    "    (2, \"Bob\", None, \"bob@example.com\"),  # Missing age\n",
    "    (3, \"Charlie\", 200, \"charlie@example.com\"),  # Invalid age\n",
    "    (4, \"David\", 35, \"david@example.com\"),\n",
    "    (4, \"David\", 35, \"david@example.com\"),  # Duplicate record\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 1: Remove NULL values in critical columns\n",
    "cleaned_df = df.filter(col(\"age\").isNotNull())\n",
    "\n",
    "# Step 2: Validate Age (Threshold: Age should be between 18-100)\n",
    "validated_df = cleaned_df.withColumn(\n",
    "    \"age\", when((col(\"age\") < 18) | (col(\"age\") > 100), lit(None)).otherwise(col(\"age\"))\n",
    ")\n",
    "\n",
    "# Step 3: Deduplicate Data (Remove Duplicates)\n",
    "deduplicated_df = validated_df.dropDuplicates()\n",
    "\n",
    "# Step 4: Flagging Invalid Data (Missing Age)\n",
    "flagged_df = deduplicated_df.withColumn(\n",
    "    \"data_quality_flag\", when(col(\"age\").isNull(), lit(\"Invalid Age\")).otherwise(lit(\"Valid\"))\n",
    ")\n",
    "\n",
    "# Show Data Quality Checked Data\n",
    "flagged_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b581011d-fe02-4339-92e2-53efa73a0ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗4. How do you design scalable data architectures that can handle increasing data volumes? Consider a simple example and impliment practically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab7a3a8-da7b-44ed-bf2d-4332d4a63c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Raw JSON Data (Simulating Data Lake Storage)\n",
    "json_data = [\n",
    "    ('{\"timestamp\":\"2024-02-25 12:00:01\", \"user\":\"Alice\", \"event\":\"login\", \"location\":\"US\"}',),\n",
    "    ('{\"timestamp\":\"2024-02-25 12:05:10\", \"user\":\"Bob\", \"event\":\"purchase\", \"amount\": 250.50, \"location\":\"UK\"}',),\n",
    "    ('{\"timestamp\":\"2024-02-25 12:10:20\", \"user\":\"Charlie\", \"event\":\"logout\", \"location\":\"IN\"}',)\n",
    "]\n",
    "\n",
    "columns = [\"raw_json\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(json_data, columns)\n",
    "\n",
    "# Step 1: Extract Key Fields from JSON (Schema Normalization)\n",
    "structured_df = df.select(\n",
    "    json_tuple(col(\"raw_json\"), \"timestamp\", \"user\", \"event\", \"amount\", \"location\") \\\n",
    "    .alias(\"timestamp\", \"user\", \"event\", \"amount\", \"location\")\n",
    ")\n",
    "\n",
    "# Step 2: Convert Data Types for Analysis\n",
    "structured_df = structured_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# Step 3: Store in a Scalable Format (Parquet for Efficient Querying)\n",
    "structured_df.write.mode(\"overwrite\").parquet(\"/mnt/data/scalable_data.parquet\")\n",
    "\n",
    "# Step 4: Read & Analyze Data from Scalable Storage\n",
    "optimized_df = spark.read.parquet(\"/mnt/data/scalable_data.parquet\")\n",
    "\n",
    "# Show Scalable Data Output\n",
    "optimized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9962b8-6a12-4ef8-b088-649da399e236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗5. How do you implement data lineage tracking in your data workflows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50f3c85-9d09-48bd-ad41-239f14077b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Data (Simulating Raw Ingestion from a Source File)\n",
    "raw_data = [\n",
    "    (1, \"Alice\", 25, \"alice@example.com\"),\n",
    "    (2, \"Bob\", 30, \"bob@example.com\"),\n",
    "    (3, \"Charlie\", None, \"charlie@example.com\"),  # Missing age\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "\n",
    "# Creating DataFrame (Raw Data)\n",
    "raw_df = spark.createDataFrame(raw_data, columns) \\\n",
    "    .withColumn(\"source_file\", lit(\"s3://data-lake/raw/customers.csv\")) \\\n",
    "    .withColumn(\"ingested_at\", current_timestamp())  # Capture source details\n",
    "\n",
    "# Step 1: Data Cleaning (Remove NULLs)\n",
    "cleaned_df = raw_df.filter(col(\"age\").isNotNull()) \\\n",
    "    .withColumn(\"transformation\", lit(\"Removed NULLs in age column\")) \\\n",
    "    .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Step 2: Hashing Emails for Privacy (Data Masking)\n",
    "masked_df = cleaned_df.withColumn(\"email\", lit(\"[MASKED]\")) \\\n",
    "    .withColumn(\"transformation\", lit(\"Masked email column\"))\n",
    "\n",
    "# Step 3: Store Lineage Metadata in Audit Table\n",
    "lineage_df = masked_df.select(\n",
    "    \"id\", \"name\", \"age\", \"email\", \"source_file\", \"ingested_at\", \"processed_at\", \"transformation\"\n",
    ")\n",
    "\n",
    "# Show Data Lineage Table\n",
    "lineage_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8676de7-e664-489f-bcda-5dc49994047d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗6. What factors do you consider when choosing an ETL tool for a project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b965404-daee-4e72-ae78-744d2da445cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Data (Simulating Extraction from MySQL)\n",
    "mysql_data = [\n",
    "    (1, \"Alice\", 25, \"alice@example.com\"),\n",
    "    (2, \"Bob\", None, \"bob@example.com\"),  # Missing age\n",
    "    (3, \"Charlie\", 40, \"charlie@example.com\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "\n",
    "# Creating DataFrame (Extracted Data)\n",
    "df = spark.createDataFrame(mysql_data, columns)\n",
    "\n",
    "# Step 1: Transformation - Remove NULL values in 'age' column\n",
    "transformed_df = df.filter(col(\"age\").isNotNull())\n",
    "\n",
    "# Step 2: Write Processed Data to S3 (Simulating Load)\n",
    "s3_path = \"s3://abduldbtlearn/processed-data/customers.parquet\"\n",
    "transformed_df.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "print(f\"✅ Data successfully saved to {s3_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2efe21e0-1063-4542-a3c5-ef75a1a99e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗7. How do you handle late-arriving data in your processing pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b372f54-dab6-4c87-b180-6e5e4b954740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema for Event Data (String for event_time, will be converted later)\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), False),\n",
    "    StructField(\"event_time\", StringType(), False)  # Keep as String, convert later\n",
    "])\n",
    "\n",
    "# Sample Late-Arriving Event Data\n",
    "event_data = [\n",
    "    (\"E101\", \"login\", \"2024-02-25 12:00:01\"),  # On-time event\n",
    "    (\"E102\", \"purchase\", \"2024-02-25 12:00:10\"),  # On-time event\n",
    "    (\"E103\", \"logout\", \"2024-02-25 12:05:00\"),  # Late event (arriving after the expected window)\n",
    "]\n",
    "\n",
    "# Creating DataFrame (Keep event_time as String initially)\n",
    "df = spark.createDataFrame(event_data, schema=schema)\n",
    "\n",
    "# Step 1: Convert `event_time` from String to Timestamp\n",
    "df = df.withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Step 2: Assigning Current Processing Time (Simulating Late Arrival)\n",
    "df = df.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "# Step 3: Windowing & Watermarking to Handle Late Events\n",
    "windowed_df = df.withWatermark(\"event_time\", \"5 minutes\") \\\n",
    "    .groupBy(window(col(\"event_time\"), \"5 minutes\"), col(\"event_type\")) \\\n",
    "    .count()\n",
    "\n",
    "# Show Late-Arriving Data Processing Output\n",
    "windowed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8f63a8-6f36-4019-917a-972231d33fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗8. Can you explain your approach to partitioning data in storage systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4c2281-77f7-4eaf-aa52-5317bfcff194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Orders Data\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-02-25\", 500),\n",
    "    (2, \"Bob\", \"2024-02-24\", 300),\n",
    "    (3, \"Charlie\", \"2024-02-24\", 700),\n",
    "    (4, \"David\", \"2024-02-23\", 150),\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"customer_name\", \"order_date\", \"amount\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(orders_data, columns)\n",
    "\n",
    "# Step 1: Partition Data by Order Date\n",
    "partitioned_path = \"s3://abduldbtlearn/partitioned-orders/\"\n",
    "df.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(partitioned_path)\n",
    "\n",
    "print(f\"✅ Data successfully partitioned and saved to {partitioned_path}\")\n",
    "\n",
    "# Step 2: Read Partitioned Data for Query Optimization\n",
    "partitioned_df = spark.read.parquet(partitioned_path)\n",
    "\n",
    "# Show Partitioned Data\n",
    "partitioned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd827fca-a3b7-4ebe-8b11-650ce07353c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗9. How do you manage and utilize metadata in your data engineering projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdbf9ff-ff1c-4a55-94cd-e3df7772f87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "# Sample Orders Data\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-02-25\", 500),\n",
    "    (2, \"Bob\", \"2024-02-24\", 300),\n",
    "    (3, \"Charlie\", \"2024-02-23\", 700),\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"customer_name\", \"order_date\", \"amount\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(orders_data, columns)\n",
    "\n",
    "# Step 1: Define Schema for Metadata\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"table_name\", StringType(), False),\n",
    "    StructField(\"schema\", StringType(), False),\n",
    "    StructField(\"row_count\", IntegerType(), False),\n",
    "    StructField(\"storage_location\", StringType(), False),\n",
    "    StructField(\"extracted_at\", TimestampType(), False)  # Explicitly define timestamp type\n",
    "])\n",
    "\n",
    "# Step 2: Extract Metadata (Use Python `datetime.now()` instead of `current_timestamp()`)\n",
    "metadata_values = [(\n",
    "    \"orders\",\n",
    "    str(df.schema),\n",
    "    df.count(),\n",
    "    \"s3://your-bucket/orders.parquet\",\n",
    "    datetime.now()  # Use Python's datetime.now() instead of Spark's current_timestamp()\n",
    ")]\n",
    "\n",
    "# Step 3: Create Metadata DataFrame\n",
    "metadata_df = spark.createDataFrame(metadata_values, schema=metadata_schema)\n",
    "\n",
    "# Step 4: Add Spark `current_timestamp()` for real-time tracking\n",
    "metadata_df = metadata_df.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "\n",
    "display(metadata_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134b56be-95d2-40f4-8744-7c6e5485fe5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ❗10. Describe a scenario where you integrated data from multiple disparate sources. What challenges did you face, and how did you overcome them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d66c5d-c84c-4bf9-ac6e-ede70c8c0754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract Data from MySQL (Simulated)\n",
    "mysql_data = [\n",
    "    (1, \"Alice\", 25, \"alice@example.com\"),\n",
    "    (2, \"Bob\", 30, \"bob@example.com\")\n",
    "]\n",
    "mysql_columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "mysql_df = spark.createDataFrame(mysql_data, mysql_columns).withColumn(\"source\", lit(\"MySQL\"))\n",
    "\n",
    "# Step 2: Read JSON Data from AWS S3 (Simulated)\n",
    "s3_data = [\n",
    "    (3, \"Charlie\", 28, \"charlie@example.com\"),\n",
    "    (4, \"David\", 35, \"david@example.com\")\n",
    "]\n",
    "s3_columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "s3_df = spark.createDataFrame(s3_data, s3_columns).withColumn(\"source\", lit(\"AWS S3\"))\n",
    "\n",
    "# Step 3: Simulate Real-Time Kafka Events (User Logs)\n",
    "kafka_data = [\n",
    "    (5, \"Eve\", 27, \"eve@example.com\"),\n",
    "    (6, \"Frank\", 32, \"frank@example.com\")\n",
    "]\n",
    "kafka_columns = [\"id\", \"name\", \"age\", \"email\"]\n",
    "kafka_df = spark.createDataFrame(kafka_data, kafka_columns).withColumn(\"source\", lit(\"Kafka\"))\n",
    "\n",
    "# Step 4: Merge All Data Sources into a Unified Dataset\n",
    "merged_df = mysql_df.union(s3_df).union(kafka_df)\n",
    "\n",
    "# Step 5: Remove Duplicates Based on ID\n",
    "cleaned_df = merged_df.dropDuplicates([\"id\"])\n",
    "\n",
    "# Show Integrated Data\n",
    "cleaned_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Scenario Based Practicals 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
