{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e8496c8-efc9-435d-a4d7-4082c70f5ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1️⃣ How would you design an ETL pipeline to handle daily incremental data updates efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea73618-240f-4d91-9f42-a8338eab39f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+------+\n| id|       name|      date|amount|\n+---+-----------+----------+------+\n|  1|   John Doe|2024-02-20|  1000|\n|  2| Jane Smith|2024-02-19|  1500|\n|  3|Alice Brown|2024-02-21|  2000|\n|  4| Bob Martin|2024-02-21|  1800|\n+---+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample Data: Creating Historical Data (Simulating Existing Dataset)\n",
    "historical_data = [\n",
    "    (1, \"John Doe\", \"2024-02-20\", 1000),\n",
    "    (2, \"Jane Smith\", \"2024-02-19\", 1500),\n",
    "]\n",
    "\n",
    "# Sample Data: Creating Incremental Data (Simulating New Daily Data)\n",
    "incremental_data = [\n",
    "    (3, \"Alice Brown\", \"2024-02-21\", 2000),\n",
    "    (4, \"Bob Martin\", \"2024-02-21\", 1800),\n",
    "    (2, \"Jane Smith\", \"2024-02-19\", 1500),  # Duplicate Entry (Should be handled)\n",
    "]\n",
    "\n",
    "# Creating DataFrames\n",
    "columns = [\"id\", \"name\", \"date\", \"amount\"]\n",
    "historical_df = spark.createDataFrame(historical_data, columns)\n",
    "incremental_df = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "# Merge Incremental Data with Historical Data (Removing Duplicates)\n",
    "merged_df = historical_df.union(incremental_df).dropDuplicates([\"id\"])\n",
    "\n",
    "# Show the Final Updated Dataset\n",
    "merged_df.show()\n",
    "\n",
    "# Save the Updated Data as Parquet (Simulating Load Phase)\n",
    "merged_df.write.mode(\"overwrite\").parquet(\"updated_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee970ffb-b785-4d69-8817-e45c5569408e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2️⃣ APIs often change over time. How would you manage schema evolution in your data pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e7b2a5-6c74-474f-949e-d6ff4cc3d411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----------+\n| id|   name| age|       city|\n+---+-------+----+-----------+\n|  1|  Alice|  25|       NULL|\n|  2|    Bob|  30|       NULL|\n|  3|Charlie|  27|   New York|\n|  4|  David|NULL|Los Angeles|\n+---+-------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample Old API Data (Initial Schema)\n",
    "old_data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "]\n",
    "\n",
    "old_columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Sample New API Data (Schema Change - 'city' added, 'age' missing in some rows)\n",
    "new_data = [\n",
    "    (3, \"Charlie\", 27, \"New York\"),\n",
    "    (4, \"David\", None, \"Los Angeles\"),  # 'age' is missing\n",
    "]\n",
    "\n",
    "# Define Column Names with Schema Change Handling\n",
    "new_columns = [\"id\", \"name\", \"age\", \"city\"]\n",
    "\n",
    "# Creating DataFrames\n",
    "old_df = spark.createDataFrame(old_data, old_columns)\n",
    "new_df = spark.createDataFrame(new_data, new_columns)\n",
    "\n",
    "# Merge DataFrames Using Union (Handling Missing Columns)\n",
    "merged_df = old_df.unionByName(new_df, allowMissingColumns=True)\n",
    "\n",
    "# Show the Final Schema Evolved DataFrame\n",
    "merged_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0353ca46-495b-47bb-94ef-138746cab545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3️⃣ Describe a time you had difficulty merging datasets. How did you solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baea6d9d-009f-4dc3-bb05-edc482156c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n| id|name_from_df1|age_from_df1|name_from_df2|age_from_df2|       city|\n+---+-------------+------------+-------------+------------+-----------+\n|  1|Alice Johnson|          25|Alice Johnson|          25|   New York|\n|  2|    Bob Smith|          30|         NULL|        NULL|       NULL|\n|  3|         NULL|        NULL|Charlie Brown|          28|Los Angeles|\n+---+-------------+------------+-------------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample Dataset 1 (Different Column Names, Age as String)\n",
    "data1 = [\n",
    "    (1, \"Alice Johnson\", \"25\"),\n",
    "    (2, \"Bob Smith\", \"30\"),\n",
    "]\n",
    "\n",
    "columns1 = [\"user_id\", \"full_name\", \"age\"]\n",
    "\n",
    "# Sample Dataset 2 (Different Column Names, Age as Integer, Missing Data)\n",
    "data2 = [\n",
    "    (1, \"Alice Johnson\", 25, \"New York\"),\n",
    "    (3, \"Charlie Brown\", 28, \"Los Angeles\"),  # No matching ID in data1\n",
    "]\n",
    "\n",
    "columns2 = [\"id\", \"name\", \"age\", \"city\"]\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Step 1: Standardize Column Names and Convert Age to Integer\n",
    "df1 = df1.withColumnRenamed(\"user_id\", \"id\").withColumnRenamed(\"full_name\", \"name\").withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "\n",
    "# Step 2: Perform an Outer Join to Include All Records\n",
    "merged_df = df1.alias(\"df1\").join(df2.alias(\"df2\"), on=\"id\", how=\"outer\")\n",
    "\n",
    "# Step 3: Explicitly Select and Rename Columns to Avoid Ambiguity\n",
    "final_df = merged_df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"df1.name\").alias(\"name_from_df1\"),\n",
    "    col(\"df1.age\").alias(\"age_from_df1\"),\n",
    "    col(\"df2.name\").alias(\"name_from_df2\"),\n",
    "    col(\"df2.age\").alias(\"age_from_df2\"),\n",
    "    col(\"df2.city\")\n",
    ")\n",
    "\n",
    "# Show the Final Merged Dataset\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bde2bf9-71b0-427d-b233-795369887c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5️⃣ How do you ensure that your data pipelines are fault-tolerant and can recover from errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849e16e0-e690-4c76-8631-17dab474198b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  1|Alice| 25|\n|  2|  Bob| 30|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Simulated Data (One Row Has an Error)\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", None),  # Simulating a faulty row with a NULL value\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Function to Process Data with Error Handling & Retries\n",
    "def process_data(df, max_retries=3):\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            # Check for NULL values and filter out faulty records\n",
    "            processed_df = df.filter(col(\"age\").isNotNull())\n",
    "\n",
    "            # Simulating Checkpointing (Saving Progress)\n",
    "            processed_df.write.mode(\"overwrite\").parquet(\"dbfs:/mnt/processed_data.parquet\")\n",
    "\n",
    "            # Successfully processed, break the loop\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {e}. Retrying ({retry_count+1}/{max_retries})...\")\n",
    "            retry_count += 1\n",
    "            time.sleep(2)  # Exponential backoff simulation\n",
    "\n",
    "# Run the Fault-Tolerant Data Processing\n",
    "process_data(df)\n",
    "\n",
    "# Load the Successfully Processed Data\n",
    "processed_df = spark.read.parquet(\"dbfs:/mnt/processed_data.parquet\")\n",
    "\n",
    "# Show the Processed Data (Without Faulty Rows)\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f02211e1-1d4d-4c1b-9687-b39a07b342cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6️⃣ Can you discuss a complex data modeling problem you've encountered and how you addressed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c76e37-1c14-4230-8285-bb01e6213a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+----------+----------+-----------+\n|customer_id| name|    address|start_date|  end_date|active_flag|\n+-----------+-----+-----------+----------+----------+-----------+\n|          1|Alice|123 Main St|2023-01-01|      NULL|          Y|\n|          2|  Bob| 456 Oak St|2023-01-01|2025-02-25|          N|\n|          2|  Bob|789 Pine St|2025-02-25|      NULL|          Y|\n+-----------+-----+-----------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#  SCD2 Implementation\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema Explicitly\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"address\", StringType(), False),\n",
    "    StructField(\"start_date\", StringType(), False),\n",
    "    StructField(\"end_date\", StringType(), True),  # Allow NULLs\n",
    "    StructField(\"active_flag\", StringType(), False),\n",
    "])\n",
    "\n",
    "# Simulated Initial Customer Data (Existing Data Warehouse Table)\n",
    "customer_data = [\n",
    "    (1, \"Alice\", \"123 Main St\", \"2023-01-01\", None, \"Y\"),\n",
    "    (2, \"Bob\", \"456 Oak St\", \"2023-01-01\", None, \"Y\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame for Existing Customer Table\n",
    "customer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "# Define Schema for Incoming Data\n",
    "new_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"address\", StringType(), False),\n",
    "])\n",
    "\n",
    "# New Incoming Data (ETL Load - Bob Changed Address)\n",
    "new_data = [\n",
    "    (1, \"Alice\", \"123 Main St\"),  # No Change\n",
    "    (2, \"Bob\", \"789 Pine St\"),  # Address Changed\n",
    "]\n",
    "\n",
    "# Create DataFrame for New Incoming Data\n",
    "new_df = spark.createDataFrame(new_data, schema=new_schema)\n",
    "\n",
    "# Step 1: Identify Changes (Join Existing and New Data)\n",
    "joined_df = customer_df.alias(\"old\").join(new_df.alias(\"new\"), \"customer_id\", \"left\")\n",
    "\n",
    "# Step 2: Mark Old Records as Inactive if Address Changed\n",
    "updated_existing_df = joined_df.withColumn(\n",
    "    \"end_date\", when(col(\"old.address\") != col(\"new.address\"), current_date()).otherwise(col(\"old.end_date\"))\n",
    ").withColumn(\n",
    "    \"active_flag\", when(col(\"old.address\") != col(\"new.address\"), lit(\"N\")).otherwise(col(\"old.active_flag\"))\n",
    ").select(\"old.customer_id\", \"old.name\", \"old.address\", \"old.start_date\", \"end_date\", \"active_flag\")\n",
    "\n",
    "# Step 3: Insert New Record for Updated Customers\n",
    "new_records_df = joined_df.filter(col(\"old.address\") != col(\"new.address\")).select(\n",
    "    col(\"new.customer_id\"),\n",
    "    col(\"new.name\"),\n",
    "    col(\"new.address\"),\n",
    "    current_date().alias(\"start_date\"),\n",
    "    lit(None).alias(\"end_date\"),\n",
    "    lit(\"Y\").alias(\"active_flag\")\n",
    ")\n",
    "\n",
    "# Step 4: Merge Active and New Records\n",
    "final_scd2_df = updated_existing_df.union(new_records_df)\n",
    "\n",
    "# Show the Final SCD Type 2 Table\n",
    "final_scd2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd24a43d-aeac-4757-8c24-3a08d38cbb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7️⃣ What are the challenges of processing real-time data, and how have you addressed them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca27f3b-799a-44e2-b0bc-c121c740e570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n|user_id|event_type|          timestamp|\n+-------+----------+-------------------+\n|    101|     click|2024-02-25 12:00:01|\n|    102|  purchase|2024-02-25 12:00:05|\n|    103|      view|2024-02-25 12:00:10|\n+-------+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define Schema for Incoming Streaming Data\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Simulating a Streaming Source (In Real Case, Kafka or Socket would be used)\n",
    "streaming_data = [\n",
    "    (101, \"click\", \"2024-02-25 12:00:01\"),\n",
    "    (102, \"purchase\", \"2024-02-25 12:00:05\"),\n",
    "    (103, \"view\", \"2024-02-25 12:00:10\"),\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"event_type\", \"timestamp\"]\n",
    "\n",
    "# Creating DataFrame (Mock Streaming Data)\n",
    "streaming_df = spark.createDataFrame(streaming_data, columns)\n",
    "\n",
    "# Process Streaming Data (Convert Timestamp)\n",
    "processed_df = streaming_df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Show the Processed Streaming Data\n",
    "processed_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09eacc7-535b-45db-a648-30b2d15aadaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8️⃣ How do you implement data governance policies in your data engineering projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a62c2b9-3fad-4585-b0c7-6410e0ce6421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+---+-----------------+------------+\n|user_id| name|            email|age|     processed_by|processed_at|\n+-------+-----+-----------------+---+-----------------+------------+\n|    101|Alice|      [PROTECTED]| 25|ETL Pipeline v1.0|  2024-02-25|\n|    102|  Bob|No Email Provided| 30|ETL Pipeline v1.0|  2024-02-25|\n+-------+-----+-----------------+---+-----------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define Schema with Governance Controls\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), False),  # Mandatory field\n",
    "    StructField(\"name\", StringType(), False),  # No NULL allowed\n",
    "    StructField(\"email\", StringType(), True),  # Nullable field\n",
    "    StructField(\"age\", IntegerType(), True)  # Age can be NULL but should be >=18\n",
    "])\n",
    "\n",
    "# Sample Data (Including Some Bad Data)\n",
    "raw_data = [\n",
    "    (101, \"Alice\", \"alice@example.com\", 25),\n",
    "    (102, \"Bob\", None, 30),  # Missing email\n",
    "    (103, \"Charlie\", \"charlie@example.com\", 15),  # Invalid age\n",
    "    (None, \"David\", \"david@example.com\", 40),  # Missing user_id (violates schema)\n",
    "]\n",
    "\n",
    "# Step 1: Remove Rows Where Required Fields Are NULL\n",
    "filtered_data = [row for row in raw_data if row[0] is not None and row[1] is not None]\n",
    "\n",
    "# Create DataFrame After Removing Bad Records\n",
    "df = spark.createDataFrame(filtered_data, schema=schema)\n",
    "\n",
    "# Step 2: Data Quality Checks (Filter Out Age < 18)\n",
    "validated_df = df.filter(col(\"age\").isNull() | (col(\"age\") >= 18))\n",
    "\n",
    "# Step 3: Mask Sensitive Data (e.g., Emails for Non-Admins)\n",
    "masked_df = validated_df.withColumn(\n",
    "    \"email\", when(col(\"email\").isNotNull(), lit(\"[PROTECTED]\")).otherwise(lit(\"No Email Provided\")))\n",
    "\n",
    "# Step 4: Add Metadata for Data Lineage\n",
    "metadata_df = masked_df.withColumn(\"processed_by\", lit(\"ETL Pipeline v1.0\")) \\\n",
    "                        .withColumn(\"processed_at\", lit(\"2024-02-25\"))\n",
    "\n",
    "# Show the Governed Data\n",
    "metadata_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c992163-ffd8-4ab1-b6bd-c1b2db12968d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 🔟 What security measures do you implement to protect sensitive data in your pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16c0d2c-ea4d-4971-8b6d-f708b02dd282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+--------------------+--------------------+------------+\n|user_id|   name|      email|         credit_card|        processed_by|processed_at|\n+-------+-------+-----------+--------------------+--------------------+------------+\n|    101|  Alice|[PROTECTED]|5d444387a5d39f5de...|Secure ETL Pipeli...|  2024-02-25|\n|    102|    Bob|[PROTECTED]|05e819efb49067336...|Secure ETL Pipeli...|  2024-02-25|\n|    103|Charlie|[PROTECTED]|f0a6cbb7aca2df9f6...|Secure ETL Pipeli...|  2024-02-25|\n+-------+-------+-----------+--------------------+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample Data with Sensitive Information\n",
    "data = [\n",
    "    (101, \"Alice\", \"alice@example.com\", \"1234-5678-9012-3456\"),\n",
    "    (102, \"Bob\", \"bob@example.com\", \"9876-5432-1098-7654\"),\n",
    "    (103, \"Charlie\", \"charlie@example.com\", \"1111-2222-3333-4444\")\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"name\", \"email\", \"credit_card\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 1: Masking Sensitive Data (Emails)\n",
    "masked_df = df.withColumn(\n",
    "    \"email\", when(col(\"email\").isNotNull(), lit(\"[PROTECTED]\")).otherwise(col(\"email\"))\n",
    ")\n",
    "\n",
    "# Step 2: Encrypting Credit Card Numbers (SHA-256 Hashing)\n",
    "encrypted_df = masked_df.withColumn(\n",
    "    \"credit_card\", sha2(col(\"credit_card\"), 256)  # Securely hashing credit card numbers\n",
    ")\n",
    "\n",
    "# Step 3: Adding Audit Columns\n",
    "secure_df = encrypted_df.withColumn(\"processed_by\", lit(\"Secure ETL Pipeline v1.0\")) \\\n",
    "                        .withColumn(\"processed_at\", lit(\"2024-02-25\"))\n",
    "\n",
    "# Show the Secured Data\n",
    "secure_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Scenario Based Practicals 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}